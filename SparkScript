import sys
from pyspark.sql import SparkSession

from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext

from pyspark.sql.functions import *
from pyspark.sql import functions as F, Window

# Define your AWS settings
region = "region"  # Replace with your AWS region
aws_account_id = "54545454545"  # Replace with your AWS account ID
table_bucket_name = "table-bucket-name"  # Replace with your S3 Table Bucket name
s3_catalog_name = f"{aws_account_id}:s3tablescatalog/{table_bucket_name}"

# Get job parameters
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'STAGING_BUCKET',
    'TABLE_BUCKET_ARN',
    'DATABASE_NAME',
    'TABLE_NAME',
    'PRIMARY_KEY'
])
# Configure the Spark session
spark = SparkSession.builder \
    .appName("AccessS3TableBucket") \
    .config("spark.sql.catalog.s3_tables", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.s3_tables.type", "rest") \
    .config("spark.sql.catalog.s3_tables.uri", f"https://glue.{region}.amazonaws.com/iceberg") \
    .config("spark.sql.catalog.s3_tables.warehouse", f"{aws_account_id}:s3tablescatalog/{table_bucket_name}") \
    .config("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog") \
    .config("spark.sql.catalog.glue_catalog.io-impl", "org.apache.iceberg.aws.s3.S3FileIO") \
    .config("spark.sql.catalog.s3_tables.rest.sigv4-enabled", "true") \
    .config("spark.sql.catalog.s3_tables.rest.signing-name", "glue") \
    .config("spark.sql.catalog.s3_tables.rest.signing-region", region) \
    .getOrCreate()

print("Spark session configured with Iceberg catalog.")

# Read from staging bucket
staging_path = f"s3://{args['STAGING_BUCKET']}/"
staging_df = spark.read.parquet(staging_path)

# Perform upsert using MERGE INTO
staging_df.createOrReplaceTempView("staging_data")
window = Window.partitionBy("id").orderBy(F.desc("time"))

staging_dedup = (
    staging_df
    .withColumn("rn", F.row_number().over(window))
    .filter("rn = 1")
    .drop("rn")
)
staging_dedup.createOrReplaceTempView("staging_dedup")

merge_query = f"""
MERGE INTO s3_tables.{args['DATABASE_NAME']}.{args['TABLE_NAME']} target
USING staging_dedup source
ON target.{args['PRIMARY_KEY']} = source.{args['PRIMARY_KEY']}
WHEN MATCHED THEN UPDATE SET target.time = source.time, updated_at = current_timestamp
WHEN NOT MATCHED THEN INSERT (id, time, updated_at) VALUES (source.id,source.time,current_timestamp)
"""

spark.sql(merge_query)

table_df = spark.sql(f"SELECT * FROM s3_tables.mynamepsace.mytable LIMIT 10")
table_df.show()
print(table_df.show())
